{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6571f144",
   "metadata": {},
   "source": [
    "# Final Project - Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64d7e6",
   "metadata": {},
   "source": [
    "This notebook serves as the final code utilized for the ASL classification project for EEL5840.  \n",
    "**Team - FML_Party**   \n",
    "**Members - Darian Jennings and Ashley Hart**\n",
    "\n",
    "**Model used - VGG19**\n",
    "\n",
    "* You should expect the test dataset to have the same format as the training data: $270,000\\times M$ ```numpy``` array, where $M$ is the number of test samples.\n",
    "* This means that *any* pre-processing applied in the training data should also be applied in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6cdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EASY-TO-MODIFY PARAMETERS\n",
    "\n",
    "# Setup path to save model\n",
    "save_path = '/blue/eel5840/darian.jennings/final_proj/'\n",
    "# Setup path to load model\n",
    "model_load_path = '/blue/eel5840/darian.jennings/final_proj/opt_model.pth'\n",
    "\n",
    "model_load1 = '/Users/darian.jennings/Desktop/UF/FML/FML_final_code/model_early_fidelity.pth'\n",
    "model_load2 = '/Users/darian.jennings/Desktop/UF/FML/FML_final_code/model_early_perceptual.pth'\n",
    "\n",
    "\n",
    "# Desired learning rate\n",
    "learning_Rate = 0.0001\n",
    "# Desired weight decay\n",
    "weight_Decay = 1e-05\n",
    "\n",
    "# Standard IMAGENET vals -- refer to references\n",
    "imagenet_means = [0.485, 0.456, 0.406]\n",
    "imagenet_stds = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff555cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darian.jennings/anaconda3/envs/EEL5840/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/darian.jennings/anaconda3/envs/EEL5840/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <9B280146-BBD7-3F77-9873-F9740F2A5329> /Users/darian.jennings/anaconda3/envs/EEL5840/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <F2FE5CF8-5B5B-3FAD-ADF8-C77D90F49FC9> /Users/darian.jennings/anaconda3/envs/EEL5840/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "from torchvision.transforms import transforms as T\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.models import VGG19_Weights\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_train = np.load('data_train.npy')\n",
    "labels_train = np.load('labels_train.npy')\n",
    "\n",
    "data_train.shape, labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AutoAugment transform \n",
    "# - automatically augments data based on a given auto-augmentation policy - IMAGENET\n",
    "augmenter = AutoAugment(policy=AutoAugmentPolicy.IMAGENET)\n",
    "\n",
    "# Resize, augment,...\n",
    "# NOTE - In PyTorch, T.ToTensor() is a transformation that converts a PIL image or numpy array to a \n",
    "# tensor and scales the values to the range [0.0, 1.0] \n",
    "# NOTE - Normalize expects Tensor - so convert to tensor then normalize (*order matters*)\n",
    "\n",
    "# create a Tensor dataset (performs transformations AND augmentation)\n",
    "class TensorDataset(Dataset):    \n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data \n",
    "        self.labels = labels \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[:, idx].reshape(300, 300, 3)\n",
    "        image = Image.fromarray(image) \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx] \n",
    "        return image, label\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    augmenter,\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(imagenet_means, imagenet_stds)\n",
    "])\n",
    "\n",
    "dataset = TensorDataset(data_train, labels_train, transform=preprocess)\n",
    "print(\"Created Tensor Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split sizes for training-validation-test, ---- use 70-15-15 rule ----\n",
    "train_size = int(0.7 * len(dataset)) \n",
    "temp_size = len(dataset) - train_size \n",
    "val_size = int(0.5 * temp_size) \n",
    "test_size = temp_size - val_size\n",
    "\n",
    "# Split dataset into training-validation-test using sizes (random_split)\n",
    "train_dataset, temp_dataset = random_split(dataset, [train_size, temp_size])\n",
    "val_dataset, test_dataset = random_split(temp_dataset, [val_size, test_size])\n",
    "\n",
    "# Use DataLoader - load data into model - flexible for memory constraints\n",
    "train_dataflow = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataflow = DataLoader(val_dataset, batch_size=256)\n",
    "test_dataflow = DataLoader(test_dataset, batch_size=256)\n",
    "print(\"Created Dataflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of pre-trained VGG19 model\n",
    "def pretrainedVGG19(num_classes):\n",
    "    model = vgg19(weights=VGG19_Weights.DEFAULT)           # pre-trained ImageNet weights\n",
    "    num_ftrs = model.classifier[6].in_features             # extract n_ftrs in output layer\n",
    "    model.classifier[6] = nn.Linear(num_ftrs, num_classes) # n_output_features= n_classes\n",
    "    return model\n",
    "\n",
    "# Define device to utilize and num_of_classes for model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Call model with give number of classes\n",
    "nclasses = 9\n",
    "model = pretrainedVGG19(nclasses)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss, add weight decay to optimizer (standard is 1e_05) - L2 penalty term\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_Rate, weight_decay=weight_Decay)\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Track epoch with highest accuracy for epochs 1-N\n",
    "highest_val_accuracy = 0\n",
    "\n",
    "# Track which indices were marked as the unknown class - should be 0 at end of training since they are all known\n",
    "# Store vals for train, loss, and accuracies respectively\n",
    "unknown_indices = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize dictionary to store information about best model\n",
    "best_model_info = {\n",
    "    'epoch': None,\n",
    "    'train_loss': None,\n",
    "    'train_accuracy': None,\n",
    "    'val_loss': None,\n",
    "    'val_accuracy': None,\n",
    "}\n",
    "\n",
    "# Collect garbage\n",
    "collected = gc.collect()\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # reset for each epoch\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    for images, labels in tqdm(train_dataflow, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
    "        images, labels = images.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # raw output scores\n",
    "        outputs = model(images)\n",
    "        # calculate the probabilities\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        #max_prob, _ = torch.max(probabilities, dim=1)\n",
    "        #print(max_prob)\n",
    "\n",
    "        # check if all probabilities are below 0.1\n",
    "        unknown = (probabilities < 0.1).all(dim=1)\n",
    "        # get the predicted classes\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # assign -1 to the unknown class\n",
    "        predicted[unknown] = -1\n",
    "        unknown_indices.extend(i for i, x in enumerate(predicted) if x == -1)\n",
    "        \n",
    "        # calculate loss petween predicted outputs and labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        # set grads to zero, make suere we donâ€™t accumulate gradients from previous iterations\n",
    "        optimizer.zero_grad()\n",
    "        # computes the gradients of the loss function\n",
    "        loss.backward()\n",
    "        # updates the parameters of the neural network\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    # calculate validation loss and accuracy\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0 \n",
    "    # deactivate autograd engine - prevent updates & data leakage during validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataflow:\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    # check if current accuracy is higher than the highest accuracy\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "            \n",
    "        # Send info to dictionary\n",
    "        best_model_info['epoch'] = epoch\n",
    "        best_model_info['train_loss'] = train_loss / len(train_dataflow)\n",
    "        best_model_info['train_accuracy'] = train_accuracy\n",
    "        best_model_info['val_loss'] = val_loss / len(val_dataflow)\n",
    "        best_model_info['val_accuracy'] = val_accuracy\n",
    "        best_model_info['model_state_dict'] = model.state_dict()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Train Acc: {train_accuracy:.2f}%, Train Loss: {train_loss/len(train_dataflow)},Val Acc: {val_accuracy:.2f}%, Val Loss: {val_loss/len(val_dataflow)}\")\n",
    "\n",
    "# Save the best model --- information is stored in dictionary (best_model_info)\n",
    "torch.save(best_model_info['model_state_dict'], os.path.join(save_path, 'opt_model.pth'))\n",
    "# Pop off model.dict -- for printing purposes (clean)\n",
    "best_model_info.popitem()\n",
    "print(\"---TRAINING COMPLETE---\")\n",
    "print(\"Unknown indices: \", unknown_indices)\n",
    "print(\"Best model info: \", best_model_info)\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves - training vs validation - for loss & accuracy\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axs[0].set_title(\"Training and Validation Loss\")\n",
    "axs[0].plot(train_losses,label=\"train\")\n",
    "axs[0].plot(val_losses,label=\"val\")\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Training and Validation Accuracy\")\n",
    "axs[1].plot(train_accuracies,label=\"train\")\n",
    "axs[1].plot(val_accuracies,label=\"val\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c598733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model instance (if not previously called) and load the saved model from path\n",
    "model = pretrainedVGG19(9) \n",
    "model.load_state_dict(torch.load(model_load1))\n",
    "\n",
    "# Move the model on the same device as the data, either CPU or GPU, for the model to process data\n",
    "model = model.to(device) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cd460",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_loss = 0, 0\n",
    "correct, total = 0, 0\n",
    "unknown_indices = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataflow:\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the probabilities\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        #max_prob, _ = torch.max(probabilities, dim=1)\n",
    "        #print(max_prob)    \n",
    "        # creates a boolean tensor unknown where each element is True if all \n",
    "        # probabilities in the corresponding row of the probabilities tensor are less than 0.5, \n",
    "        # and False otherwise\n",
    "        unknown = (probabilities < 0.3).all(dim=1)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        #print(predicted)\n",
    "        # classify unknown images to the unknown class (-1)\n",
    "        predicted[unknown] = -1\n",
    "        unknown_indices.extend(i for i, x in enumerate(predicted) if x == -1)\n",
    "            \n",
    "        test_loss += criterion(output, target).item()\n",
    "        _, predicted = torch.max(output.data, 1) \n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_loss /= len(test_dataflow.dataset) \n",
    "test_accuracy = 100 * correct / total\n",
    "y_true = target.cpu().numpy()\n",
    "y_pred = predicted.cpu().numpy()\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%, Test Loss: {test_loss:.4f}, F1-score: {f1:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}')\n",
    "print(\"---TEST COMPLETE---\")\n",
    "print(\"Unknown indices: \", set(unknown_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1438027",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
